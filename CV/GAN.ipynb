{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5, ), (0.5, )),\n",
    "    ])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"datasets\", train=True, download=True, transform=trans)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"datasets\", train=False,download=True, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2906d55ca0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAANh0lEQVR4nO3df6zddX3H8dfL/sJeYFKwtSuVKqKxOsHlCppuSw3DAYYUo2w0GekSZskGCSxmG2ExkmxxjIiETWdSR2clCFOBQLRzksaNkLHKhZRSKFuRdVh71wvUrUXgtqXv/XG/LJdyz+dezvd7zve07+cjuTnnfN/ne77vfHtf/X7v+XzP+TgiBODY95a2GwDQH4QdSIKwA0kQdiAJwg4kMbufG5vreXGchvq5SSCVV/QLHYhxT1WrFXbb50u6RdIsSX8XETeUnn+chnSOz62zSQAFm2NTx1rXp/G2Z0n6qqQLJC2XtNr28m5fD0Bv1fmb/WxJT0fEMxFxQNKdklY10xaAptUJ+xJJP530eFe17HVsr7U9YnvkoMZrbA5AHXXCPtWbAG+49jYi1kXEcEQMz9G8GpsDUEedsO+StHTS41Ml7a7XDoBeqRP2hyWdYftdtudKulTSfc20BaBpXQ+9RcQh21dJ+idNDL2tj4gnGusMQKNqjbNHxEZJGxvqBUAPcbkskARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IIlaUzbb3ilpv6RXJR2KiOEmmgLQvFphr3w8Ip5v4HUA9BCn8UASdcMekn5o+xHba6d6gu21tkdsjxzUeM3NAehW3dP4FRGx2/ZCSffbfioiHpj8hIhYJ2mdJJ3oBVFzewC6VOvIHhG7q9sxSfdIOruJpgA0r+uw2x6yfcJr9yV9QtK2phoD0Kw6p/GLJN1j+7XX+VZE/KCRrgA0ruuwR8Qzks5ssBcAPcTQG5AEYQeSIOxAEoQdSIKwA0k08UGYFF747Mc61t552dPFdZ8aW1SsHxifU6wvuaNcn7/rxY61w1ueLK6LPDiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLPP0J/88bc61j499PPyyqfX3PjKcnnnoZc61m557uM1N370+vHYaR1rQzf9UnHd2Zseabqd1nFkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkHNG/SVpO9II4x+f2bXtN+sVnzulYe/5D5f8zT9pe3sc/f7+L9bkf+p9i/cYP3t2xdt5bXy6u+/2Xji/WPzm/82fl63o5DhTrm8eHivWVxx3setvv+f4Vxfp71z7c9Wu3aXNs0r7YO+UvFEd2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCz7PP0NB3Nxdq9V77xHqr62/esbJj7S9WLCtv+1/K33l/48r3dNHRzMx++XCxPrR1tFg/+YG7ivVfmdv5+/bn7yx/F/+xaNoju+31tsdsb5u0bIHt+23vqG5P6m2bAOqayWn8NySdf8SyayVtiogzJG2qHgMYYNOGPSIekLT3iMWrJG2o7m+QdHGzbQFoWrdv0C2KiFFJqm4Xdnqi7bW2R2yPHNR4l5sDUFfP342PiHURMRwRw3M0r9ebA9BBt2HfY3uxJFW3Y821BKAXug37fZLWVPfXSLq3mXYA9Mq04+y279DEN5efYnuXpC9IukHSt21fLulZSZf0skmUHfrvPR1rQ3d1rknSq9O89tB3X+iio2bs+f2PFesfmFv+9f3S3vd1rC37+2eK6x4qVo9O04Y9IlZ3KB2d30IBJMXlskAShB1IgrADSRB2IAnCDiTBR1zRmtmnLS3Wv3LdV4r1OZ5VrH/nlt/sWDt59KHiuscijuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7GjNU3+0pFj/yLzyVNZPHChPR73gyZfedE/HMo7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+zoqfFPfqRj7dHP3DzN2uUZhP7g6quL9bf+64+nef1cOLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs6Onnr2g8/HkeJfH0Vf/53nF+vwfPFasR7Gaz7RHdtvrbY/Z3jZp2fW2f2Z7S/VzYW/bBFDXTE7jvyHp/CmW3xwRZ1U/G5ttC0DTpg17RDwgaW8fegHQQ3XeoLvK9tbqNP+kTk+yvdb2iO2RgxqvsTkAdXQb9q9JOl3SWZJGJd3U6YkRsS4ihiNieM40H2wA0DtdhT0i9kTEqxFxWNLXJZ3dbFsAmtZV2G0vnvTwU5K2dXougMEw7Ti77TskrZR0iu1dkr4gaaXtszQxlLlT0hW9axGD7C0nnFCsX/brD3as7Tv8SnHdsS++u1ifN/5wsY7XmzbsEbF6isW39qAXAD3E5bJAEoQdSIKwA0kQdiAJwg4kwUdcUcuO6z9QrH/vlL/tWFu149PFdedtZGitSRzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlR9L+/+9Fifevv/HWx/pNDBzvWXvyrU4vrztNosY43hyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOHtys5f8crF+zef/oVif5/Kv0KWPXdax9vZ/5PPq/cSRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJz9GOfZ5X/iM7+3q1i/5PgXivXb9y8s1hd9vvPx5HBxTTRt2iO77aW2f2R7u+0nbF9dLV9g+37bO6rbk3rfLoBuzeQ0/pCkz0XE+yV9VNKVtpdLulbSpog4Q9Km6jGAATVt2CNiNCIere7vl7Rd0hJJqyRtqJ62QdLFPeoRQAPe1Bt0tpdJ+rCkzZIWRcSoNPEfgqQp/3izvdb2iO2Rgxqv2S6Abs047LaPl3SXpGsiYt9M14uIdRExHBHDczSvmx4BNGBGYbc9RxNBvz0i7q4W77G9uKovljTWmxYBNGHaoTfblnSrpO0R8eVJpfskrZF0Q3V7b086RD1nvq9Y/vOFt9V6+a9+8ZJi/W2PPVTr9dGcmYyzr5B0maTHbW+pll2niZB/2/blkp6VVP5XB9CqacMeEQ9Kcofyuc22A6BXuFwWSIKwA0kQdiAJwg4kQdiBJPiI6zFg1vL3dqytvbPe5Q/L119ZrC+77d9qvT76hyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOPsx4Kk/7PzFvhfNn/GXCk3p1H8+UH5CRK3XR/9wZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnPwq8ctHZxfqmi24qVOc32wyOWhzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJmczPvlTSNyW9Q9JhSesi4hbb10v6rKTnqqdeFxEbe9VoZrtXzCrW3zm7+7H02/cvLNbn7Ct/np1Psx89ZnJRzSFJn4uIR22fIOkR2/dXtZsj4ku9aw9AU2YyP/uopNHq/n7b2yUt6XVjAJr1pv5mt71M0oclba4WXWV7q+31tqf8biTba22P2B45qPF63QLo2ozDbvt4SXdJuiYi9kn6mqTTJZ2liSP/lBdoR8S6iBiOiOE5mle/YwBdmVHYbc/RRNBvj4i7JSki9kTEqxFxWNLXJZU/rQGgVdOG3bYl3Sppe0R8edLyxZOe9ilJ25pvD0BTZvJu/ApJl0l63PaWatl1klbbPksToy87JV3Rg/5Q01++sLxYf+i3lhXrMfp4g92gTTN5N/5BSZ6ixJg6cBThCjogCcIOJEHYgSQIO5AEYQeSIOxAEo4+Trl7ohfEOT63b9sDstkcm7Qv9k41VM6RHciCsANJEHYgCcIOJEHYgSQIO5AEYQeS6Os4u+3nJP3XpEWnSHq+bw28OYPa26D2JdFbt5rs7bSIePtUhb6G/Q0bt0ciYri1BgoGtbdB7Uuit271qzdO44EkCDuQRNthX9fy9ksGtbdB7Uuit271pbdW/2YH0D9tH9kB9AlhB5JoJey2z7f977aftn1tGz10Ynun7cdtb7E90nIv622P2d42adkC2/fb3lHdTjnHXku9XW/7Z9W+22L7wpZ6W2r7R7a3237C9tXV8lb3XaGvvuy3vv/NbnuWpP+QdJ6kXZIelrQ6Ip7sayMd2N4paTgiWr8Aw/ZvSHpR0jcj4oPVshsl7Y2IG6r/KE+KiD8dkN6ul/Ri29N4V7MVLZ48zbikiyX9nlrcd4W+flt92G9tHNnPlvR0RDwTEQck3SlpVQt9DLyIeEDS3iMWr5K0obq/QRO/LH3XobeBEBGjEfFodX+/pNemGW913xX66os2wr5E0k8nPd6lwZrvPST90PYjtte23cwUFkXEqDTxyyNpYcv9HGnaabz76Yhpxgdm33Uz/XldbYR9qu/HGqTxvxUR8auSLpB0ZXW6ipmZ0TTe/TLFNOMDodvpz+tqI+y7JC2d9PhUSbtb6GNKEbG7uh2TdI8GbyrqPa/NoFvdjrXcz/8bpGm8p5pmXAOw79qc/ryNsD8s6Qzb77I9V9Klku5roY83sD1UvXEi20OSPqHBm4r6PklrqvtrJN3bYi+vMyjTeHeaZlwt77vWpz+PiL7/SLpQE+/I/0TSn7XRQ4e+3i3psernibZ7k3SHJk7rDmrijOhySSdL2iRpR3W7YIB6u03S45K2aiJYi1vq7dc08afhVklbqp8L2953hb76st+4XBZIgivogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJ/wNGNvRI2D7VDgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((test_dataset.data.float()[0]/255).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data_utils\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BatchLoadDataset(Dataset):\n",
    "\n",
    "    def __init__(self, maskPath, imgPath, imgSize, transform=None, maskExt=\"png\"):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.imgs = os.listdir(imgPath)\n",
    "        self.masks = os.listdir(maskPath)\n",
    "        self.maskExt = maskExt\n",
    "\n",
    "        self.allImgs = torch.zeros([len(self.imgs),3, imgSize,imgSize], dtype=torch.float32)\n",
    "        self.allMasks = torch.zeros([len(self.masks),1, imgSize,imgSize], dtype=torch.int8)\n",
    "\n",
    "        # for idx in tqdm(range(len(self.imgs))):\n",
    "        for idx in tqdm(range(10)):\n",
    "            \n",
    "            mask_path = os.path.join(self.maskPath, self.imgs[idx][:-3] + self.maskExt)\n",
    "            mask = io.imread(mask_path)\n",
    "\n",
    "            img_path = os.path.join(self.imgPath, self.imgs[idx])\n",
    "            image = io.imread(img_path)\n",
    "            # if len(mask.shape) < 3:\n",
    "            #     image = np.expand_dims(image, axis = 2)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                mask = self.transform(mask)\n",
    "            \n",
    "            self.allImgs[idx] = mask.squeeze(0)\n",
    "            self.allMasks[idx] = image.squeeze(0)\n",
    "\n",
    "        # self.final.to(\"cuda\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return self.allImgs[idx], self.allMasks[idx]\n",
    "    \n",
    "\n",
    "def get_data_loader(args):\n",
    "\n",
    "    if args.dataset == 'cifar':\n",
    "        trans = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root=args.dataroot, train=True, download=args.download, transform=trans)\n",
    "\n",
    "    elif args.dataset == 'stl10':\n",
    "        trans = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        train_dataset = torchvision.datasets.STL10(root=args.dataroot, split='train', download=args.download, transform=trans)\n",
    "\n",
    "    elif args.dataset == 'JSRT':\n",
    "        image_size = args.image_size\n",
    "        train_dataset = BatchLoadDataset(root=args.dataroot,imgSize = image_size, \n",
    "                                imgTransform=transforms.Compose([\n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.Resize(image_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5)*args.imgC, (0.5)*args.imgC),\n",
    "                                ]),\n",
    "                                maskTransform=transforms.Compose([\n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.Resize(image_size),\n",
    "                                    transforms.ToTensor(),\n",
    "                                ]), maskExt=\"png\")\n",
    "    \n",
    "\n",
    "    assert train_dataset\n",
    "\n",
    "    train_dataloader = data_utils.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'JSRT'\n",
    "        self.dataroot = '/content/drive/MyDrive/JSRT'\n",
    "        self.image_size = 256\n",
    "        self.batch_size = 4\n",
    "        self.download = False\n",
    "        self.imgC = 3\n",
    "\n",
    "args = Args()\n",
    "get_data_loader(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UpTranspose2d(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Doubles input image size\n",
    "    Stride assumed to be 2 and padding depends on kernel size.\n",
    "    Leaky ReLu generally used in GANs & BatchNorm does not preserve the independence between images,\n",
    "    therefore instance norm used.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ch_in, ch_out, kernelSize = 4, leak = 0.2, stride = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upTrans = nn.ConvTranspose2d(ch_in, ch_out, kernel_size=kernelSize, stride = stride, padding=(kernelSize//2 - 1))\n",
    "        self.norm = nn.InstanceNorm2d(ch_out, affine=True)\n",
    "        self.actFn = nn.LeakyReLU(leak, inplace=True)\n",
    "        \n",
    "    def forward(self, image):\n",
    "\n",
    "        x = self.upTrans(image)\n",
    "        x = self.actFn(self.norm(x))\n",
    "        return x\n",
    "\n",
    "class UpSampleConv(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Doubles input image size\n",
    "    Nearest Neighbour Upsample doubles image size followed by two Conv Layers as used in ResNet\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ch_in, ch_out,kernelSize = 3, mode = 'nearest'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=kernelSize, padding = (kernelSize-1)//2)\n",
    "        self.norm1 = nn.BatchNorm2d(ch_out, affine=True)\n",
    "        self.actFn1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=kernelSize, padding = (kernelSize-1)//2)\n",
    "        self.norm2 = nn.BatchNorm2d(ch_out, affine=True)\n",
    "        self.actFn2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, image):\n",
    "\n",
    "        x = self.actFn1(self.norm1(self.conv1(image)))\n",
    "        x = self.actFn2(self.norm2(self.conv2(x)))\n",
    "        return x\n",
    "    \n",
    "class GeneratorTrans(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, kernelSize=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gen = nn.ModuleList([UpTranspose2d(channels[i], channels[i+1], kernelSize) for i in range(len(channels) - 2)])\n",
    "        self.output = nn.ConvTranspose2d(channels[-2], channels[-1], kernel_size=kernelSize, stride = 2, padding=(kernelSize//2 - 1))\n",
    "\n",
    "    def forward(self, image):\n",
    "            \n",
    "        for block in self.gen:\n",
    "            image = block(image)\n",
    "            print(image.shape)\n",
    "    \n",
    "        output = torch.tanh(self.output(image))\n",
    "        return output\n",
    "\n",
    "class GeneratorUpSample(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, kernelSize=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gen = nn.ModuleList([UpSampleConv(channels[i], channels[i+1], kernelSize) for i in range(len(channels) - 2)])\n",
    "        self.upSample = nn.Upsample(scale_factor=2, mode='nearest') \n",
    "        self.output = nn.Conv2d(channels[-2], channels[-1], kernel_size=kernelSize, padding = (kernelSize-1)//2)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        for block in self.gen:\n",
    "            image = block(self.upSample(image))\n",
    "            print(image.shape)\n",
    "\n",
    "        output = torch.tanh(self.output(self.upSample(image)))\n",
    "        return output\n",
    "\n",
    "class DownConv2d(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Halves input image size\n",
    "    Using stride = 2 instead of Pooling.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ch_in, ch_out,kernelSize):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(ch_in, ch_out, kernel_size=kernelSize, stride = 2, padding = kernelSize//2 - 1)\n",
    "        self.norm = nn.InstanceNorm2d(ch_out, affine=True)\n",
    "        self.actFn = nn.ReLU()\n",
    "        \n",
    "    def forward(self, image):\n",
    "\n",
    "        x = self.actFn(self.norm(self.conv(image)))\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, kernelSize=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dis = nn.ModuleList([DownConv2d(channels[i], channels[i+1], kernelSize) for i in range(len(channels) - 2)])\n",
    "        self.out = nn.Conv2d(in_channels=channels[-2], out_channels=channels[-1], kernel_size=kernelSize)\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        for block in self.dis:\n",
    "            image = block(image)\n",
    "            # print(image.shape)\n",
    "        return self.out(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_len = 100\n",
    "n_classes = 1\n",
    "img_size = 256\n",
    "n_channels = 3\n",
    "channelsG = [latent_len, 64, 128, 128, 128, 64, 32, 32,n_channels]\n",
    "channelsD = [n_channels, 64, 128, 128, 128, 256, 512, 1]\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "bs = 4\n",
    "epochs = 1\n",
    "D_epochs = 1\n",
    "\n",
    "\n",
    "# G = GeneratorUpSample(channelsG)\n",
    "G = GeneratorTrans(channelsG)\n",
    "D = Discriminator(channelsD)\n",
    "\n",
    "optimizerG = torch.optim.Adam(G.parameters(), lr=0.001)\n",
    "optimizerD = torch.optim.Adam(D.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    # Training Discriminator\n",
    "\n",
    "    for _ in range(D_epochs):\n",
    "        \n",
    "        optimizerD.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.rand(bs,latent_len,1,1)\n",
    "            fake_imgs = G(z)\n",
    "\n",
    "        print(fake_imgs.shape)\n",
    "\n",
    "        output = D(fake_imgs).flatten()\n",
    "        labels = torch.tensor([0]*bs).float()\n",
    "        lossF = criterion(output, labels)\n",
    "\n",
    "        real_imgs = (torch.rand((bs,n_channels,img_size,img_size)) - 0.5)/0.5\n",
    "        output = D(real_imgs).flatten()\n",
    "        labels = torch.tensor([1]*bs).float()\n",
    "        lossR = criterion(output, labels)\n",
    "\n",
    "        lossD = lossR + lossF\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "    ## Training Generator\n",
    "    optimizerG.zero_grad()\n",
    "    z = torch.rand(bs,latent_len,1,1)\n",
    "    fake_imgs = G(z)\n",
    "    output = D(fake_imgs).flatten()\n",
    "\n",
    "    label = torch.tensor([1]*bs).float()\n",
    "    lossG = criterion(output, label)\n",
    "    lossG.backward()\n",
    "    optimizerG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDown = [n_channels, 64, 128, 128, 128, 256, latent_len]\n",
    "channelsUp = [latent_len, 256, 128, 128, 128, 64 ,n_channels]\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, kernelSizeUp=3, kernelSizeDown=4):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.dis = nn.ModuleList([DownConv2d(channels[i], channels[i+1], kernelSize=4) for i in range(len(channels) - 2)])\n",
    "        self.out = nn.Conv2d(in_channels=channels[-2], out_channels=channels[-1], kernel_size=kernelSizeDown)\n",
    "\n",
    "        \n",
    "        self.gen = nn.ModuleList([UpSampleConv(channels[i], channels[i+1], kernelSize=3) for i in range(len(channels) - 2)])\n",
    "        self.upSample = nn.Upsample(scale_factor=2, mode='nearest') \n",
    "        self.output = nn.Conv2d(channels[-2], channels[-1], kernel_size=kernelSizeUp, padding = (kernelSizeUp-1)//2)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        li = []\n",
    "        for block in self.dis:\n",
    "            image = block(image)\n",
    "            li.append(image)\n",
    "        self.out(image)\n",
    "\n",
    "            \n",
    "        for block in self.gen:\n",
    "            image = block(self.upSample(image))\n",
    "            print(image.shape)\n",
    "\n",
    "        output = torch.tanh(self.output(self.upSample(image)))\n",
    "        return output\n",
    "\n",
    "        return self.out(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet Architecture\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(256, 512, 3)\n",
    "        self.conv8 = nn.Conv2d(512, 512, 3)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        self.upconv1 = nn.ConvTranspose2d(512,256,2, stride = 2)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(512, 256, 3)\n",
    "        self.conv10 = nn.Conv2d(256, 256, 3)\n",
    "        self.bn9 = nn.BatchNorm2d(256)\n",
    "        self.bn10 = nn.BatchNorm2d(256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256,128,2, stride = 2)\n",
    "\n",
    "        self.conv11 = nn.Conv2d(256, 128, 3)\n",
    "        self.conv12 = nn.Conv2d(128, 128, 3)\n",
    "        self.bn11 = nn.BatchNorm2d(128)\n",
    "        self.bn12 = nn.BatchNorm2d(128)\n",
    "        self.upconv3 = nn.ConvTranspose2d(128,64,2, stride = 2)\n",
    "\n",
    "        self.conv13 = nn.Conv2d(128, 64, 3)\n",
    "        self.conv14 = nn.Conv2d(64, 64, 3)\n",
    "        self.conv15 = nn.Conv2d(64, 1, 1)\n",
    "        self.bn13 = nn.BatchNorm2d(64)\n",
    "        self.bn14 = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x1 = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x1)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x2 = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x2)\n",
    "\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x3 = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x3)\n",
    "\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        x = self.upconv1(x)\n",
    "\n",
    "        lpad = (x3.shape[2] - x.shape[2])//2\n",
    "        tpad = (x3.shape[3] - x.shape[3])//2\n",
    "        x = torch.cat((x3[:, :,lpad:-lpad, tpad:-tpad],x), 1)\n",
    "        x = self.bn9(F.relu(self.conv9(x)))\n",
    "        x = self.bn10(F.relu(self.conv10(x)))\n",
    "        x = self.upconv2(x)\n",
    "\n",
    "        lpad = (x2.shape[2] - x.shape[2])//2\n",
    "        tpad = (x2.shape[3] - x.shape[3])//2\n",
    "        x = torch.cat((x2[:, :,lpad:-lpad, tpad:-tpad],x), 1)\n",
    "        x = self.bn11(F.relu(self.conv11(x)))\n",
    "        x = self.bn12(F.relu(self.conv12(x)))\n",
    "        x = self.upconv3(x)\n",
    "\n",
    "\n",
    "        lpad = (x1.shape[2] - x.shape[2])//2\n",
    "        tpad = (x1.shape[3] - x.shape[3])//2\n",
    "        x = torch.cat((x1[:, :,lpad:-lpad, tpad:-tpad],x), 1)\n",
    "        x = self.bn13(F.relu(self.conv13(x)))\n",
    "        x = self.bn14(F.relu(self.conv14(x)))\n",
    "        x = self.conv15(x)\n",
    "        x = x[:,:,2:-2,2:-2]\n",
    "\n",
    "        return x.squeeze(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1af58a0c390f807515830a18bbb19ac451fbe3aa00c4c733482807097ac6a02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
