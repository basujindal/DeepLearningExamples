{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, edim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.edim = edim\n",
    "        self.dk = self.edim//self.h\n",
    "        self.key = nn.Linear(self.edim,self.edim)\n",
    "        self.query = nn.Linear(self.edim,self.edim)\n",
    "        self.value = nn.Linear(self.edim,self.edim)\n",
    "        self.linear = nn.Linear(self.edim,self.edim)\n",
    "        \n",
    "\n",
    "    def forward(self, key, value,query):\n",
    "\n",
    "        bs = key.shape[0]\n",
    "        nwords_key = key.shape[1]\n",
    "        nwords_query = query.shape[1]\n",
    "\n",
    "        k = self.key(key).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        q = self.query(query).reshape(bs, nwords_query, self.h, self.dk).transpose(1,2)\n",
    "        v = self.value(value).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        x = torch.einsum('bhmd,bhnd -> bhmn',(q,k))\n",
    "\n",
    "        x = F.softmax(x/(self.dk)**0.5, dim=3)\n",
    "\n",
    "        x = torch.einsum('bhmn,bhnv -> bhmv', (x,v))\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        x = x.reshape(bs, nwords_query, -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, 4*hdim)\n",
    "        self.fc2 = nn.Linear(4*hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, embed):\n",
    "\n",
    "        x = self.multiHeadAttention(embed, embed, embed)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + embed)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer1)))\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        return subLayer2\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder\n",
    "    nx: number of transformer blocks\n",
    "    edim: embedding dimension\n",
    "    h: number of heads\n",
    "    hdim: hidden dimension\n",
    "    '''\n",
    "    def __init__(self, nx, edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformers = nn.ModuleList([EncoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, embed):\n",
    "        for block in self.transformers:\n",
    "            embed = block(embed)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    Positional Embedding\n",
    "    '''\n",
    "    def __init__(self, edim, npatches):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(npatches, edim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Classification Head\n",
    "    Takes mean of the output of the transformer and passes it through a linear layer\n",
    "    '''\n",
    "    def __init__(self, edim, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(edim, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.linear(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    '''\n",
    "    ViT\n",
    "    nx: number of transformer blocks\n",
    "    edim: embedding dimension\n",
    "    h: number of heads\n",
    "    hdim: hidden dimension\n",
    "    dropout: dropout probability\n",
    "    n_classes: number of classes\n",
    "    patch_dim: dimension of the patch\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nx, edim, h, hdim, dropout, n_classes, patch_dim, npatches):\n",
    "        super().__init__()\n",
    "        self.posEmbedding = PositionalEmbedding(edim, npatches)\n",
    "        self.embedding = nn.Linear(patch_dim,edim)\n",
    "        self.encoder = Encoder(nx, edim, h, hdim,dropout)\n",
    "        self.classificationHead = ClassificationHead(edim, n_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, patches):\n",
    "\n",
    "        pos_embed = self.posEmbedding(torch.arange(patches.shape[1], device=patches.device))\n",
    "        # print(pos_embed.shape)\n",
    "        src_embed = self.embedding(patches) + pos_embed\n",
    "        # print(src_embed.shape)\n",
    "        src_embed = self.dropout(src_embed)\n",
    "        encoded = self.encoder(src_embed)\n",
    "        output = self.classificationHead(encoded)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in Million:  53.333224\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m pos_embed \u001b[39m=\u001b[39m pos_embed\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 28\u001b[0m output \u001b[39m=\u001b[39m vit(x)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 133\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, patches)\u001b[0m\n\u001b[1;32m    131\u001b[0m pos_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposEmbedding(torch\u001b[39m.\u001b[39marange(patches\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], device\u001b[39m=\u001b[39mpatches\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m    132\u001b[0m \u001b[39m# print(pos_embed.shape)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m src_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(patches) \u001b[39m+\u001b[39m pos_embed\n\u001b[1;32m    134\u001b[0m \u001b[39m# print(src_embed.shape)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m src_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(src_embed)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "img_size = 64\n",
    "patch_size = 16\n",
    "n_channels = 3\n",
    "patch_dim = n_channels * patch_size * patch_size\n",
    "npatches = img_size // patch_size\n",
    "\n",
    "\n",
    "vit = ViT(n_classes=1000, nx=6, edim=768, h=8, hdim=1024, dropout=0.1, patch_dim=patch_dim, npatches=npatches).to(device)\n",
    "\n",
    "print(\"number of parameters in Million: \", sum(p.numel() for p in vit.parameters() if p.requires_grad)/1e6)\n",
    "## try on random data\n",
    "\n",
    "bs = 2\n",
    "npatches = 49\n",
    "n_classes = 1000\n",
    "x = torch.randn(bs, npatches, patch_dim)\n",
    "\n",
    "## positional embedding is same for all patches\n",
    "pos_embed = torch.randn(npatches, patch_dim)\n",
    "## expand to batch size\n",
    "pos_embed = pos_embed.unsqueeze(0).expand(bs, npatches, patch_dim)\n",
    "\n",
    "## forward pass\n",
    "x = x.to(device)\n",
    "pos_embed = pos_embed.to(device)\n",
    "output = vit(x)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
