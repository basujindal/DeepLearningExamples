{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, edim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.edim = edim\n",
    "        self.dk = self.edim//self.h\n",
    "        self.key = nn.Linear(self.edim,self.edim)\n",
    "        self.query = nn.Linear(self.edim,self.edim)\n",
    "        self.value = nn.Linear(self.edim,self.edim)\n",
    "        self.linear = nn.Linear(self.edim,self.edim)\n",
    "        \n",
    "\n",
    "    def forward(self, key, value,query):\n",
    "\n",
    "        bs = key.shape[0]\n",
    "        nwords_key = key.shape[1]\n",
    "        nwords_query = query.shape[1]\n",
    "\n",
    "        k = self.key(key).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        q = self.query(query).reshape(bs, nwords_query, self.h, self.dk).transpose(1,2)\n",
    "        v = self.value(value).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        x = torch.einsum('bhmd,bhnd -> bhmn',(q,k))\n",
    "\n",
    "        x = F.softmax(x/(self.dk)**0.5, dim=3)\n",
    "\n",
    "        x = torch.einsum('bhmn,bhnv -> bhmv', (x,v))\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        x = x.reshape(bs, nwords_query, -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, 4*hdim)\n",
    "        self.fc2 = nn.Linear(4*hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, embed):\n",
    "\n",
    "        x = self.multiHeadAttention(embed, embed, embed)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + embed)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer1)))\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        return subLayer2\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder\n",
    "    nx: number of transformer blocks\n",
    "    edim: embedding dimension\n",
    "    h: number of heads\n",
    "    hdim: hidden dimension\n",
    "    '''\n",
    "    def __init__(self, nx, edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformers = nn.ModuleList([EncoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, embed):\n",
    "        for block in self.transformers:\n",
    "            embed = block(embed)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    Positional Embedding\n",
    "    '''\n",
    "    def __init__(self, edim, npatches):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(npatches, edim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Classification Head\n",
    "    Takes mean of the output of the transformer and passes it through a linear layer\n",
    "    '''\n",
    "    def __init__(self, edim, n_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(edim, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return self.linear(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    '''\n",
    "    ViT\n",
    "    nx: number of transformer blocks\n",
    "    edim: embedding dimension\n",
    "    h: number of heads\n",
    "    hdim: hidden dimension\n",
    "    dropout: dropout probability\n",
    "    n_classes: number of classes\n",
    "    patch_dim: dimension of the patch\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nx, edim, h, hdim, dropout, n_classes, patch_dim, npatches):\n",
    "        super().__init__()\n",
    "        self.posEmbedding = PositionalEmbedding(edim, npatches)\n",
    "        self.embedding = nn.Linear(patch_dim,edim)\n",
    "        self.encoder = Encoder(nx, edim, h, hdim,dropout)\n",
    "        self.classificationHead = ClassificationHead(edim, n_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, patches):\n",
    "\n",
    "        pos_embed = self.posEmbedding(torch.arange(patches.shape[1], device=patches.device))\n",
    "        src_embed = self.embedding(patches) + pos_embed\n",
    "        src_embed = self.dropout(src_embed)\n",
    "        encoded = self.encoder(src_embed)\n",
    "        output = self.classificationHead(encoded)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in Million:  53.333224\n",
      "torch.Size([2, 1000])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "img_size = 64\n",
    "patch_size = 16\n",
    "n_channels = 3\n",
    "patch_dim = n_channels * patch_size * patch_size\n",
    "npatches = img_size // patch_size\n",
    "\n",
    "\n",
    "vit = ViT(n_classes=1000, nx=6, edim=768, h=8, hdim=1024, dropout=0.1, patch_dim=patch_dim, npatches=npatches).to(device)\n",
    "\n",
    "print(\"number of parameters in Million: \", sum(p.numel() for p in vit.parameters() if p.requires_grad)/1e6)\n",
    "## try on random data\n",
    "\n",
    "bs = 2\n",
    "n_classes = 1000\n",
    "x = torch.randn(bs, npatches, patch_dim)\n",
    "\n",
    "## positional embedding is same for all patches\n",
    "pos_embed = torch.randn(npatches, patch_dim)\n",
    "## expand to batch size\n",
    "pos_embed = pos_embed.unsqueeze(0).expand(bs, npatches, patch_dim)\n",
    "\n",
    "## forward pass\n",
    "x = x.to(device)\n",
    "pos_embed = pos_embed.to(device)\n",
    "output = vit(x)\n",
    "print(output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
