{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi\n",
    "# !git clone https://github.com/basujindal/CodeSnippets.git\n",
    "# !pip install transformers wandb\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import pickle\n",
    "from re import sub\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import time\n",
    "# from custom_transformer import CustomTransformer\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4442492\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/en_tokenized.pkl', 'rb') as f:\n",
    "    en_tokens = pickle.load(f)\n",
    "\n",
    "with open('datasets/de_tokenized.pkl', 'rb') as f:\n",
    "    de_tokens = pickle.load(f)\n",
    "\n",
    "print(len(en_tokens))\n",
    "assert(len(de_tokens) == len(en_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_de = Tokenizer.from_file(\"tokenizer_de_25000_start_token_SOS.json\")\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en_25000_start_token_SOS.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h, edim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = h\n",
    "        self.edim = edim\n",
    "        self.dk = self.edim//self.h\n",
    "        self.key = nn.Linear(self.edim,self.edim)\n",
    "        self.query = nn.Linear(self.edim,self.edim)\n",
    "        self.value = nn.Linear(self.edim,self.edim)\n",
    "        self.linear = nn.Linear(self.edim,self.edim)\n",
    "        \n",
    "\n",
    "    def forward(self, key, value,query, mask = None):\n",
    "\n",
    "        bs = key.shape[0]\n",
    "        nwords_key = key.shape[1]\n",
    "        nwords_query = query.shape[1]\n",
    "\n",
    "        k = self.key(key).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        q = self.query(query).reshape(bs, nwords_query, self.h, self.dk).transpose(1,2)\n",
    "        v = self.value(value).reshape(bs, nwords_key, self.h, self.dk).transpose(1,2)\n",
    "        x = torch.einsum('bhmd,bhnd -> bhmn',(q,k))\n",
    "        \n",
    "        if mask != None:\n",
    "            x = x.masked_fill(mask == False, float(\"-1e10\"))\n",
    "\n",
    "        x = F.softmax(x/(self.dk)**0.5, dim=3)\n",
    "\n",
    "        x = torch.einsum('bhmn,bhnv -> bhmv', (x,v))\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        x = x.reshape(bs, nwords_query, -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, hdim)\n",
    "        self.fc2 = nn.Linear(hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "\n",
    "        x = self.multiHeadAttention(src_embed,src_embed,src_embed, src_mask)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + src_embed)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer1)))\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        return subLayer2\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,edim, h, hdim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.maskedMultiHeadAttention = MultiHeadAttention(h, edim)\n",
    "        self.norm1 = nn.LayerNorm(edim)\n",
    "        self.norm2 = nn.LayerNorm(edim)\n",
    "        self.norm3 = nn.LayerNorm(edim)\n",
    "        self.fc1 = nn.Linear(edim, hdim)\n",
    "        self.fc2 = nn.Linear(hdim, edim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt_embed, src_encoded, src_mask, tgt_mask):\n",
    "\n",
    "        x = self.maskedMultiHeadAttention(tgt_embed, tgt_embed, tgt_embed, tgt_mask)\n",
    "        x = self.dropout1(x)\n",
    "        subLayer1 = self.norm1(x + tgt_embed)\n",
    "\n",
    "        x = self.multiHeadAttention(src_encoded, src_encoded, subLayer1, src_mask)\n",
    "        x = self.dropout2(x)\n",
    "        subLayer2 = self.norm2(x + subLayer1)\n",
    "\n",
    "        x = self.fc2(self.relu(self.fc1(subLayer2)))\n",
    "        x = self.dropout3(x)\n",
    "        subLayer3 = self.norm3(x + subLayer2)\n",
    "        \n",
    "        return subLayer3\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.transformers = nn.ModuleList([EncoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, src_embed, src_mask):\n",
    "        for block in self.transformers:\n",
    "            embed = block(src_embed, src_mask)\n",
    "        return embed\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nx = nx\n",
    "        self.transformers = nn.ModuleList([DecoderBlock(edim, h, hdim,dropout) for _ in range(nx)])\n",
    "\n",
    "    def forward(self, encoded, tgt_embed, src_mask, tgt_mask):\n",
    "\n",
    "        for block in self.transformers:\n",
    "            embed = block(tgt_embed, encoded, src_mask, tgt_mask)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class CustomTransformer(nn.Module):\n",
    "    def __init__(self, nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size):\n",
    "        super().__init__()\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size,edim)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size,edim)\n",
    "        self.encoder = Encoder(nx, edim, h, hdim,dropout)\n",
    "        self.decoder = Decoder(nx, edim, h, hdim,dropout)\n",
    "        self.fc = nn.Linear(edim, tgt_vocab_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src_tokens, tgt_tokens, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed, encoded = None):\n",
    "        \n",
    "        if encoded == None:\n",
    "\n",
    "            src_embed = self.src_embedding(src_tokens) + src_pos_embed\n",
    "            src_embed = self.dropout1(src_embed)\n",
    "            encoded = self.encoder(src_embed, src_mask)\n",
    "\n",
    "        tgt_embed = self.tgt_embedding(tgt_tokens) + tgt_pos_embed\n",
    "        tgt_embed = self.dropout2(tgt_embed)\n",
    "        \n",
    "        decoded = self.decoder(encoded, tgt_embed, src_mask, tgt_mask)\n",
    "        output = self.fc(decoded)\n",
    "\n",
    "        return output, encoded\n",
    "\n",
    "    # def forward(self, src_tokens, tgt_tokens, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed):\n",
    "\n",
    "    #     src_embed = self.src_embedding(src_tokens) + src_pos_embed\n",
    "    #     src_embed = self.dropout1(src_embed)\n",
    "    #     if torch.isnan(src_embed).any():\n",
    "    #         print(\"EMBEDING IS WRONG\")\n",
    "    #         print(src_embed)\n",
    "    #     tgt_embed = self.tgt_embedding(tgt_tokens) + tgt_pos_embed\n",
    "    #     tgt_embed = self.dropout2(tgt_embed)\n",
    "    #     if torch.isnan(tgt_embed).any():\n",
    "    #         print(\"TGT IS WRONG\")\n",
    "    #         print(tgt_embed)\n",
    "    #     encoded = self.encoder(src_embed, src_mask)\n",
    "    #     if torch.isnan(encoded).any():\n",
    "    #         print(\"ENCODER IS WRONG\")\n",
    "    #         print(encoded)\n",
    "    #     decoded = self.decoder(encoded, tgt_embed, src_mask, tgt_mask)\n",
    "    #     if torch.isnan(encoded).any():\n",
    "    #         print(\"DECODER IS WRONG\")\n",
    "    #         print(encoded)\n",
    "    #     output = self.fc(decoded)\n",
    "\n",
    "    #     return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(en_tokens, de_tokens, bs, src_pad_idx, tgt_pad_idx, device, shuffle = True):\n",
    "    num_batches = len(en_tokens)//bs\n",
    "    idxs = [i for i in range(num_batches)]\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(idxs)\n",
    "\n",
    "    for i in idxs:\n",
    "        max_len = len(en_tokens[(i+1)*bs - 1])\n",
    "        en_tensor = torch.tensor([enc.tolist() + \n",
    "        [src_pad_idx]*(max_len - len(enc)) for enc in en_tokens[i*bs:(i+1)*bs]], device=device)\n",
    "        \n",
    "        en_pad = torch.tensor((en_tensor != src_pad_idx),device=device).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        tgt_lens = [len(enc) for enc in de_tokens[i*bs:(i+1)*bs]]\n",
    "        max_len= max(tgt_lens)\n",
    "        tgt = torch.tensor([enc.tolist() + \n",
    "        [tgt_pad_idx]*(max_len - len(enc)) for enc in de_tokens[i*bs:(i+1)*bs]], device=device)\n",
    "        \n",
    "        de_mask = torch.ones(bs, max_len, max_len, device=device).tril().unsqueeze(1)\n",
    "\n",
    "        labels = torch.cat([tgt[ii][1:tgt_lens[ii]] for ii in range(tgt.shape[0])], dim = 0).to(device)\n",
    "\n",
    "        yield en_tensor, en_pad, tgt, de_mask, tgt_lens, labels\n",
    "\n",
    "\n",
    "def positionEmbeding(edim, max_nwords):\n",
    "    pos_emb = torch.zeros((max_nwords, edim))\n",
    "    for pos in range(max_nwords):\n",
    "        for i in range(edim//2):\n",
    "            pos_emb[pos, 2*i] = np.sin(pos/(10000**(2*i/edim)))\n",
    "            pos_emb[pos, 2*i + 1] = np.cos(pos/(10000**(2*i/edim)))\n",
    "\n",
    "    return pos_emb\n",
    "\n",
    "def remove_large_Sentences(en_tokens, de_tokens, max_size):\n",
    "    li = []\n",
    "    for i in range(len(en_tokens)):\n",
    "        if(len(en_tokens[i]) <= max_size and len(de_tokens[i]) <= max_size):\n",
    "            li.append(i)\n",
    "\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "\n",
    "    def __init__(self, model_size, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self):\n",
    "        return self.model_size ** (-0.5) *min(self._step ** (-0.5),  self._step * self.warmup ** (-1.5))\n",
    "\n",
    "def save_model(PATH):\n",
    "    state = {\n",
    "      'state_dict': net.state_dict(),\n",
    "      'optimizer': optimizer.state_dict(),\n",
    "      'scheduler_step': scheduler._step\n",
    "      }\n",
    "    torch.save(state, PATH)\n",
    "\n",
    "\n",
    "def validate(val_en_tokens, val_de_tokens, bs):\n",
    "\n",
    "    net.val()\n",
    "    correct, total = 0, 0\n",
    "    p_bar=tqdm(total=len(val_en_tokens)//bs)\n",
    "\n",
    "    for src, src_mask, tgt, tgt_mask, tgt_lens, labels in loader(val_en_tokens, val_de_tokens, bs, src_pad_idx, tgt_pad_idx, device):\n",
    "        p_bar.update(1)\n",
    "\n",
    "        src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0)\n",
    "        tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0)\n",
    "\n",
    "        outputs, _ = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "\n",
    "        li = [outputs[ii][:tgt_lens[ii]-1] for ii in range(outputs.shape[0])]\n",
    "        probs = torch.cat(li, dim = 0)\n",
    "        predicted = torch.max(probs, 1)[1]\n",
    "        \n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "nx = 6\n",
    "edim = 512\n",
    "hdim = 2048\n",
    "h = 8\n",
    "src_vocab_size = 25000\n",
    "tgt_vocab_size = 25000\n",
    "n_epochs = 10\n",
    "bs = 32\n",
    "src_pad_idx = 0\n",
    "tgt_pad_idx = 2\n",
    "dropout = 0.1\n",
    "max_nwords = 100\n",
    "n_epochs = 1\n",
    "val_steps = 10000\n",
    "val_accu = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = CustomTransformer(nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), betas=[0.9, 0.98])\n",
    "scheduler = NoamOpt(edim,2500,optimizer)\n",
    "# scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 65000\n",
    "PATH = \"saved_models/n_steps_\" + str(step)\n",
    "state = torch.load(PATH)\n",
    "net.load_state_dict(state['state_dict'])\n",
    "optimizer.load_state_dict(state['optimizer'])\n",
    "scheduler._step = state['scheduler_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idxs = remove_large_Sentences(en_tokens, de_tokens, max_nwords)\n",
    "en_tokens = [en_tokens[i] for i in max_idxs]\n",
    "de_tokens = [de_tokens[i] for i in max_idxs]\n",
    "assert(len(de_tokens) == len(en_tokens))\n",
    "posEmb = positionEmbeding(edim, max_nwords).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = False\n",
    "\n",
    "if logging:\n",
    "    wandb.init(project=\"transformers\")\n",
    "    wandb.config = {\n",
    "    \"nsteps\": 500000,\n",
    "    \"batch_size\": bs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "net.train()\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    p_bar=tqdm(total=len(en_tokens)//bs)\n",
    "  \n",
    "    for src, src_mask, tgt, tgt_mask, tgt_lens, labels in loader(en_tokens, de_tokens, bs, src_pad_idx, tgt_pad_idx, device):\n",
    "        p_bar.update(1)\n",
    "\n",
    "        step+=1\n",
    "        src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0)\n",
    "        tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        outputs, _ = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "\n",
    "        li = [outputs[ii][:tgt_lens[ii]-1] for ii in range(outputs.shape[0])]\n",
    "        probs = torch.cat(li, dim = 0)\n",
    "        loss = criterion(probs, labels)   \n",
    "\n",
    "        if(step%100 == 0 and  np.isnan(loss.data.cpu().numpy())):\n",
    "            print(\"Loss is NaN\")\n",
    "            print(outputs)\n",
    "            break\n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.max(probs, 1)[1]\n",
    "        \n",
    "        total = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "        if step%2:\n",
    "            scheduler.step()\n",
    "\n",
    "        if logging:\n",
    "            try:\n",
    "                wandb.log(\n",
    "                    {\"loss\": loss.data,\n",
    "                    \"lr\": scheduler._rate,\n",
    "                    \"accuracy\": correct/(total+1),\n",
    "                    \"validation accuracy\": val_accu,\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if(step%5000 == 0):\n",
    "            PATH = \"saved_models/n_steps_\" + str(step)\n",
    "            save_model(PATH)\n",
    "\n",
    "        \n",
    "        if step % val_steps == 0: \n",
    "            val_accu = validate(val_en_tokens, val_de_tokens, bs)\n",
    "            print(\"step {0} | loss: {1:.4f} | Val Accuracy: {2:.3f} %\".format(epoch, loss, val_accu))\n",
    "\n",
    "            if val_accu > best_accu:\n",
    "              best_accu = val_accu \n",
    "              torch.save(net.state_dict(), 'net_val.pth')\n",
    "              print(\"Saving\")\n",
    "\n",
    "            net.train()\n",
    "\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, net, device):\n",
    "\n",
    "    net.eval()\n",
    "    posEmb = positionEmbeding(edim, max_nwords).to(device)\n",
    "\n",
    "    src_enc = tokenizer_en.encode(sentence).ids\n",
    "    src = torch.tensor(src_enc, device=device).unsqueeze(0)\n",
    "    src_mask = None\n",
    "\n",
    "    tgt_enc = [0]\n",
    "    tgt = torch.tensor(tgt_enc, device=device).unsqueeze(0)\n",
    "    tgt_mask =  torch.ones(1, len(tgt), len(tgt),device=device).tril().unsqueeze(1)\n",
    "    \n",
    "\n",
    "\n",
    "    src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0)\n",
    "    tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0)\n",
    "\n",
    "    output, encoded = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "\n",
    "    _, predicted_idx = torch.max(output.data[0], 1)\n",
    "    print(tgt)\n",
    "    print(predicted_idx.tolist())\n",
    "    print(tokenizer_de.decode(predicted_idx.tolist()))\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    while(predicted_idx[-1].tolist() != 1):\n",
    "        \n",
    "        tgt = torch.cat((tgt, predicted_idx[-1].unsqueeze(0).unsqueeze(0)), 1)\n",
    "        tgt_mask =  torch.ones(1, tgt.shape[-1], tgt.shape[-1]).tril().unsqueeze(1).to(device)\n",
    "        # print(tgt)\n",
    "        # print(\"Hello\", tgt_mask)\n",
    "        output, _ = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed, encoded)\n",
    "        _, predicted_idx = torch.max(output.data[0], 1)\n",
    "        print(predicted_idx.tolist())\n",
    "        print(tokenizer_de.decode(predicted_idx.tolist()))\n",
    "        idx+=1\n",
    "\n",
    "        if(idx == max_nwords):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx = 6\n",
    "edim = 512\n",
    "hdim = 2048\n",
    "h = 8\n",
    "src_vocab_size = 25000\n",
    "tgt_vocab_size = 25000\n",
    "pad_idx = 0\n",
    "dropout = 0.1\n",
    "max_nwords = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = CustomTransformer(nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "step = 65000\n",
    "PATH = \"saved_models/n_steps_\" + str(step)\n",
    "state = torch.load(PATH)\n",
    "net.load_state_dict(state['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Please don't do this\"\n",
    "translate(sentence, net, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Pytorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 6\n",
    "edim = 512\n",
    "hdim = 2048\n",
    "h = 8\n",
    "src_vocab_size = 25000\n",
    "tgt_vocab_size = 25000\n",
    "bs = 32\n",
    "src_pad_idx = 0\n",
    "tgt_pad_idx = 2\n",
    "dropout = 0.1\n",
    "max_nwords = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "net = CustomTransformer(nx, edim, h, hdim, dropout, src_vocab_size, tgt_vocab_size).to(device)\n",
    "\n",
    "step = 125000\n",
    "PATH = \"saved_models/n_steps_\" + str(step)\n",
    "state = torch.load(PATH)\n",
    "net.load_state_dict(state['state_dict'])\n",
    "optimizer = optim.Adam(net.parameters(), betas=[0.9, 0.98])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "posEmb = positionEmbeding(edim, max_nwords).to(device)\n",
    "\n",
    "for src, src_mask, tgt, tgt_mask, tgt_lens, labels in loader(en_tokens, de_tokens, bs, src_pad_idx, tgt_pad_idx, device):\n",
    "\n",
    "    posEmb = positionEmbeding(edim, max_nwords).to(device)\n",
    "    src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0)\n",
    "    tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.profiler.profile(\n",
    "#     schedule=torch.profiler.schedule(\n",
    "#         wait=2,\n",
    "#         warmup=2,\n",
    "#         active=6,\n",
    "#         repeat=1),\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler,\n",
    "#     with_stack=True\n",
    "# ) as profiler:\n",
    "from torch import profiler\n",
    "with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "\n",
    "    # for src, src_mask, tgt, tgt_mask, tgt_lens, labels in loader(en_tokens, de_tokens, bs, src_pad_idx, tgt_pad_idx, device):\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     src_pos_embed = posEmb[:src.shape[1]].unsqueeze(0)\n",
    "    #     tgt_pos_embed = posEmb[:tgt.shape[1]].unsqueeze(0)\n",
    "\n",
    "        # outputs, _ = net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "\n",
    "        # li = [outputs[ii][:tgt_lens[ii]-1] for ii in range(outputs.shape[0])]\n",
    "        # probs = torch.cat(li, dim = 0)\n",
    "        # loss = criterion(probs, labels)   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # break\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import profiler\n",
    "with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    net(src, tgt, src_mask, tgt_mask, src_pos_embed, tgt_pos_embed)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('misc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1af58a0c390f807515830a18bbb19ac451fbe3aa00c4c733482807097ac6a02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
